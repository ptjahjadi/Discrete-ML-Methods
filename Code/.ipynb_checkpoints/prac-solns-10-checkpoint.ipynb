{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# School of Computing and Information Systems\n",
    "### The University of Melbourne\n",
    "### COMP30027 M ACHINE L EARNING (Semester 1, 2019)\n",
    "\n",
    "Practical exercises: Week 10\n",
    "Following on from our examination of the simple voting ensemble method from last week:\n",
    "\n",
    "# 1. \n",
    "Bagging is often associated with Decision Trees, but in scikit-learn , it is a meta–classifier, that\n",
    "can be applied to any learner, for example:\n",
    "```python\n",
    ">>> from sklearn.ensemble import BaggingClassifier\n",
    ">>> from sklearn.neighbors import KNeighborsClassifier\n",
    ">>> bagging = BaggingClassifier(KNeighborsClassifier(),max_samples=0.5, max_features=0.5)\n",
    "```\n",
    "\n",
    "What are the significance of max_samples and max_features , and why might we wish to use\n",
    "values less than 1.0?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the documentation https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html.\n",
    "\n",
    "Bagging classifier builds N base classifier, each base classifier is trained/fitted on a subset of features/samples.\n",
    "For each base classifier:\n",
    "* Randomly select max_features * X.shape[1] subset of features.\n",
    "* Randomly select max_samples * X.shape[0] subset of samples.\n",
    "* Create a new X_base from the selected features and samples.\n",
    "* Fit the base classifier on X_base and y_base.\n",
    "\n",
    "Then use Voting or averaging to combine the prediction of the base classifier for X_test.\n",
    "\n",
    "**If max_features=1.0 and max_samples=1.0 then all the base classifiers will be similar so there will be no point in combining them.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. \n",
    "Stacking is one strategy that scikit-learn doesn’t directly support.\n",
    "It’s not too hard, though:\n",
    "* We need to train each of our models (using fit() ),\n",
    "* And then classify each training instance (using predict() ),\n",
    "* We build up a matrix where the instances are composed of attributes, which correspond to\n",
    "the predictions of each model on this training instance 1 .\n",
    "* We then train our final learner on this matrix of predictions.\n",
    "# (a) \n",
    "Implement a stacking classifier.\n",
    "# (b) \n",
    "Think about which classifier is most suited to being the final meta–classifier in this situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels: {'good', 'vgood', 'acc', 'unacc'}\n",
      "stacker acc: 0.9544658493870403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "class StackingClassifier():\n",
    "\n",
    "    def __init__(self, classifiers, metaclassifier):\n",
    "        self.classifiers = classifiers\n",
    "        self.metaclassifier = metaclassifier\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        for clf in self.classifiers:\n",
    "            clf.fit(X, y)\n",
    "        X_meta = self._predict_base(X)\n",
    "        self.metaclassifier.fit(X_meta, y)\n",
    "    \n",
    "    def _predict_base(self, X):\n",
    "        yhats = []\n",
    "        for clf in self.classifiers:\n",
    "            yhat = clf.predict_proba(X)\n",
    "            yhats.append(yhat)\n",
    "        yhats = np.concatenate(yhats, axis=1)\n",
    "        #print(yhats.shape)\n",
    "        assert yhats.shape[0] == X.shape[0]\n",
    "        return yhats\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_meta = self._predict_base(X)\n",
    "        yhat = self.metaclassifier.predict(X_meta)\n",
    "        return yhat\n",
    "    def score(self, X, y):\n",
    "        yhat = self.predict(X)\n",
    "        return accuracy_score(y, yhat)\n",
    "    \n",
    "\n",
    "\n",
    "classifiers = [LogisticRegression(),\n",
    "                KNeighborsClassifier(),\n",
    "                GaussianNB(),\n",
    "                MultinomialNB(),\n",
    "                DecisionTreeClassifier()]\n",
    "\n",
    "meta_classifier = DecisionTreeClassifier()\n",
    "stacker = StackingClassifier(classifiers, meta_classifier)\n",
    "\n",
    "def load_car_data(car_file):\n",
    "    X = []\n",
    "    y = []\n",
    "    with open(car_file, mode='r') as fin:\n",
    "        for line in fin:\n",
    "            atts = line.strip().split(\",\")\n",
    "            X.append(atts[:-1]) #all atts minus the last one\n",
    "            y.append(atts[-1])\n",
    "    onehot = OneHotEncoder()\n",
    "    X = onehot.fit_transform(X).toarray()\n",
    "    return X, y\n",
    "X, y = load_car_data('car.data')\n",
    "print('labels:', set(y))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "stacker.fit(X_train, y_train)\n",
    "print('stacker acc:', stacker.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A final meta-classifier for stacking should probably be non-linear, and because the features might be dependent, a naive base classifier might not be a good idea. So a decision tree or *nonlinear* SVC is a natural choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. \n",
    "Remember the UCI Car Evaluation dataset from way back?\n",
    "### (a) \n",
    "By examining the data, remind yourself in what way this is an artificial dataset. Hypothesise\n",
    "what a Decision Tree classifier built on this dataset would probably look like.\n",
    "\n",
    "The features are subjective, and artificial, so a decision tree needs to be deeper than usual to be able to predict well on the training data, which results in overfitting (not having a good performance on test data).\n",
    "\n",
    "### (b) \n",
    "Compare the training accuracy and cross-validation accuracy of a Decision Tree Classifier\n",
    "and a Random Forest on this dataset. What do you notice?\n",
    "### (c) \n",
    "Based on your observations, hypothesise what sorts of datasets Random Forests would be\n",
    "more effective/less effective than the regular deterministic Decision Tree method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dt train acc: 1.0 rf train acc: 0.9976851851851852 bagging train acc 0.9971064814814815\n",
      "dt cross-val acc: 0.9456285487730626 rf cross-val acc: 0.9068106939372588 bagging cross-val acc: 0.943726329509728\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier \n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "rf = RandomForestClassifier()\n",
    "bagging = BaggingClassifier(base_estimator=dt, max_samples = 0.8, max_features = 0.8)\n",
    "\n",
    "dt.fit(X, y)\n",
    "rf.fit(X, y)\n",
    "bagging.fit(X,y)\n",
    "\n",
    "print('dt train acc:', dt.score(X, y), 'rf train acc:', rf.score(X, y), 'bagging train acc', bagging.score(X,y))\n",
    "cv = 30\n",
    "print('dt cross-val acc:', np.mean(cross_val_score(dt, X, y, cv=cv)), 'rf cross-val acc:', np.mean(cross_val_score(rf, X, y, cv=cv)),'bagging cross-val acc:', np.mean(cross_val_score(bagging, X, y, cv=cv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
